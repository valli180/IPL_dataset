{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c288e0-62b9-44c1-86bb-3fe206462c62",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size = 0.2, random_state = 123)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, test_size = 0.2, random_state = 123)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b854e-33e5-4462-9ca2-2a81f0b2f1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "844b1cf2-ed6f-4212-bcaf-8db2a7ec6de4",
   "metadata": {},
   "source": [
    "#### Make pipeline\n",
    "\n",
    "pipe = make_pipeline(DummyClassifier(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5dfcf-1602-4923-919b-edf93b80bbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82df0e22-62bc-4e10-9f05-8fb5fa5f0b1c",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "result = pd.DataFrame(cross_validate(pipe, X_train, y_train, cv= 10,return_train_score = True, scoring = \"f1\")).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4047b9-af58-4717-b01b-46edf0187372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0e2bdcf-a72a-4669-9439-2a59d2db360a",
   "metadata": {},
   "source": [
    "### Make Column Tranformer\n",
    "<u>from sklearn.compose import ColumnTransformer, make_column_transformer\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abc9b893-0fdd-4924-b1e4-ea3ed60679e1",
   "metadata": {},
   "source": [
    "\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "ordinal_transformer_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_reg),\n",
    ")\n",
    "\n",
    "ordinal_transformer_oth = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_oth),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer_reg, ordinal_features_reg),\n",
    "    (ordinal_transformer_oth, ordinal_features_oth),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7996d7-7df5-4386-80f2-46641baa4193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193b6ec5-5cc3-44c6-889b-64c0a6d5387f",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "<u>from sklearn.metrics import ConfusionMatrixDisplay  \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6922644a-1f26-4a0f-86f8-7a2fe8cc0846",
   "metadata": {},
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "cm = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"fraud\"]   # Displayed as color plot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f4c0b-1dcc-4ac0-b078-a7fa62e89d9f",
   "metadata": {},
   "source": [
    "<u>from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a758884-e1c5-4d74-a780-280e77c0083f",
   "metadata": {},
   "source": [
    "predictions = pipe.predict(X_valid)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "\n",
    "plot_confusion_matrix_example(TN, FP, FN, TP) #plot no colors, arranged column wise, first row filled,then second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86c623-f2db-4fc2-960f-38ae024cd4b9",
   "metadata": {},
   "source": [
    "<u>from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f4b6fb9-a4a0-40b8-b879-0c0de714a1c6",
   "metadata": {},
   "source": [
    "confusion_matrix(y_train, cross_val_predict(pipe, X_train, y_train)) # in the form of an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2754630-0403-4df8-a8ec-798baed75dd4",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8e0e3-c1be-432e-bd64-8490490f9357",
   "metadata": {},
   "source": [
    "Accuracy is a good measure for balanced data, but when there is class imbalance, accuracy is not the correct metrics to be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cee65-0372-4bfd-92fc-f33d6212664c",
   "metadata": {},
   "source": [
    ".score will return accuracy. But in case of classification models, we need, precison, recall and f1 score.\n",
    "F1 score is generally used for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df114ac7-9b4c-4b64-b4a4-d8cf5277539a",
   "metadata": {},
   "source": [
    "pipe.classes_ # ? \n",
    "\n",
    "pipe_lr.named_steps[\"logisticregression\"].classes_# ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff18ea-9c75-44af-91ce-8f17c60e1dad",
   "metadata": {},
   "source": [
    "<u>from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_valid, pipe_lr.predict(X_valid), target_names=[\"non-fraud\", \"fraud\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48ea64-219c-47a6-9901-6f3dabcaac66",
   "metadata": {},
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "y_pred = pipe.predict_proba(X_test)[ : ,1] > 0.5\n",
    "\n",
    "y_pred = pipe.predict_proba(X_test)[:,1] > 0.1\n",
    "\n",
    "The above y_pred code means that even at the slightest probability of 0.1% predict y_pred to be in positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbdabd-db91-4087-b262-d4cbb3a831f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f8d3df9-fa5a-4920-8b69-b9395a89419d",
   "metadata": {},
   "source": [
    "### ROC Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd961b-9f3f-4f2c-85c0-3b360fa9a06e",
   "metadata": {},
   "source": [
    "Every prediction your classifier makes on a new data point has an associated probability. By default in scikit-learn, if this probability is above 50%, then your model would predict the data point as belonging to the positive class, and if it is lower than 50%, it would predict the negative class. In other words, the default probability threshold here is 50%. What happens if we vary this threshold, and, for each threshold, plot the model's true positive rate against the false positive rate? We get what is known as the ROC curve.\n",
    "This curve allows you to diagnose the performance of the model at different thresholds.\n",
    "The y-axis is the true positive rate and ranges from 0 to 1,and the x-axis is the false positive rate which also ranges from 0 to 1.\n",
    "A perfect classifier, then, would have a true positive rate of 1 and false positive rate of 0.\n",
    "\n",
    "\n",
    "This information can be used to extract another useful metric: the area under the ROC curve, known as the AUC. Notice that a better performing model will have a larger area under the curve.\n",
    "\n",
    "\n",
    "The ROC curve of a model that randomly guesses would be a diagonal line,\n",
    "\n",
    "\n",
    "and the area under this line\n",
    "\n",
    "\n",
    "would be 0 point 5. Therefore, having an AUC above 0 point 5 would be better than randomly guessing, while an AUC above 0 point 7 or 0 point 8 would indicate a well-performing model.\n",
    "\n",
    "\n",
    "Together with the ROC curve, the AUC allows you to easily evaluate and compare the performance of different classifiers. In order to plot the ROC, you need to first calculate the probabilities that your model generates for each data point. Scikit-learn classifiers such as the logistic regression classifier shown here have a predict proba method for this purpose. Each column contains the probabilities for the respective target values. So for the first data point, the probability of the predicted label being \"0\" is around 80%, and the probability of it being \"1\" is 20%. Since our positive class is \"1\", we're interested in the second column, and we can select that as shown here.\n",
    "\n",
    "\n",
    "To plot the ROC curve, import roc curve from sklearn dot metrics. This function takes in two arguments: The actual labels, followed by the predicted probabilities we just computed. Unpack this result into three variables: false positive rate, FPR; true positive rate, TPR; and the thresholds. We can then plot the FPR and TPR using matplotlib. The additional code here provides informative labels and plots the diagonal line which we can use to compare against.\n",
    "\n",
    "\n",
    "To compute the AUC, sklearn dot metrics has a roc_auc_score function that you can use similarly to the other metrics functions in scikit-learn.\n",
    "\n",
    "\n",
    "Now it is your turn to evaluate the performance of your classifiers using ROC curves!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1055e8f-2eb1-4b8e-b316-450a1060ecb2",
   "metadata": {},
   "source": [
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "# Import roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Calculate the roc metrics\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Print the AUC\n",
    "print(roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a4acd-8f67-4673-8d75-0804d7bf83dc",
   "metadata": {},
   "source": [
    "<u>from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "roc_plot= RocCurveDisplay.from_estimator(final_svc_model, X_test, y_test) #fit the model on train_df before this\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64589ba5-7405-40d5-b8f8-6b88e6349cb7",
   "metadata": {},
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ece952-e99b-4411-9c01-77a4cb4264dd",
   "metadata": {},
   "source": [
    "<u>from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "pr_curve = PrecisionRecallDisplay.from_estimator(final_svc_model, X_test, y_test) # fit the model on train_df before this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c25f4a-8e19-4dcf-bb5d-a246e12fdb5b",
   "metadata": {},
   "source": [
    "PR curve shows the tradeoff between precision and recall for different thresholds.This curves summarize the tradeoff between the true positive rate and the positive predictive value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d46d1-b6b8-4fea-aa7a-2e1a61881cf0",
   "metadata": {},
   "source": [
    "Increasing the threshold means,higher bar for predicting the positive case.\n",
    "Increasing the precision, we dont want to have any false positives. Precision will increase , occasionally might decrease\n",
    "recall may stay the same or go down. \n",
    "\n",
    "Decreasing the threshold means we are risking to have false positive for having higher true positive.\n",
    "recall may remain same or might increase\n",
    "\n",
    "There is always trade off between precison and recall depending on the threshold.\n",
    "Lower thershold(left of pr or ap curve) will cause low precision, increasing the threshold(moving to right) increases precision but at the expense of lowering the recall. The extreme upper right of the curve represents the ideal situation where recall = precision = 1, where there are no false positives or false negatives, which is ideal case, not possible.\n",
    "\n",
    "Ap score is the summary across all the threshold- we can find the threshold at which we have a balance between the recall and the precision from the above pr curve.\n",
    "\n",
    "Ap score measures the quality of the predict_proba\n",
    "while f1 score measures the quality of the predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65305d77-820e-4dff-a219-3fe13a08594c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bcb6b5e-0932-45b5-9509-1951f89f64fe",
   "metadata": {},
   "source": [
    "<u>from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "\n",
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6402012-2af3-4b70-912c-e704c9f12b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
